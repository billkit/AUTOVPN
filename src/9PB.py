import os
import sys
import time
import yaml
import requests
import socket
import concurrent.futures
import traceback
from collections import defaultdict, OrderedDict
from datetime import datetime, timedelta, timezone
import base64
import re
import json
import urllib.parse
from urllib.parse import unquote, urlparse, parse_qs

# ---------------- Config ----------------
REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), ".."))
OUTPUT_FILE = os.path.join(REPO_ROOT, "9PB")
SOURCES_FILE = os.path.join(REPO_ROOT, "SUB_9PB")
TEMPLATE_URL = "https://raw.githubusercontent.com/Vanic24/VPN/refs/heads/main/ClashTemplate.ini"
TEXTDB_API = "https://textdb.online/update/?key=9PB_SHFX&value={}"


def env_bool(name: str, default: bool = False) -> bool:
    value = os.getenv(name)
    if value is None:
        return default
    return value.lower() == "true"


def env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, default))
    except (TypeError, ValueError):
        return default


def env_json(name: str, default: dict) -> dict:
    raw = os.getenv(name)
    if not raw:
        return default
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return default


CN_TO_CC = env_json("CN_TO_CC", {})
USE_ONLY_GEOIP = env_bool("USE_ONLY_GEOIP", False)

USE_LATENCY = env_bool("LATENCY_FILTER", False)
LATENCY_THRESHOLD = env_int("LATENCY_THRESHOLD", 100)

USE_DUPLICATE_FILTER = env_bool("DUPLICATE_FILTER", False)

# ---------------- Inputs ----------------
use_latency_env = os.environ.get("LATENCY_FILTER", "false").lower()
USE_LATENCY = use_latency_env == "true"

try:
    LATENCY_THRESHOLD = int(os.environ.get("LATENCY_THRESHOLD", "100"))
except ValueError:
    LATENCY_THRESHOLD = 100

use_dup_env = os.environ.get("DUPLICATE_FILTER", "false").lower()
USE_DUPLICATE_FILTER = use_dup_env == "true"

# ---------------- Helper ----------------
def resolve_ip(host):
    try:
        return socket.gethostbyname(host)
    except:
        return None

def tcp_latency_ms(host, port, timeout=2.0):
    try:
        start = time.time()
        sock = socket.create_connection((host, port), timeout=timeout)
        sock.close()
        return int((time.time() - start) * 1000)
    except:
        return 9999

def deduplicate_nodes(nodes):
    """
    Remove duplicate nodes based on:
    - (server, port, uuid) OR
    - (server, port, password)

    Server string must match EXACTLY.
    """
    seen = set()
    unique_nodes = []
    removed = 0

    for n in nodes:
        server = str(n.get("server", "")).strip()
        port = int(n.get("port", 0))
        uuid = str(n.get("uuid", "")).strip()
        password = str(n.get("password", "")).strip()

        # Build key
        if uuid:
            key = ("uuid", server, port, uuid)
        elif password:
            key = ("password", server, port, password)
        else:
            # No dedup key ‚Üí keep it
            unique_nodes.append(n)
            continue

        if key in seen:
            removed += 1
            continue

        seen.add(key)
        unique_nodes.append(n)

    return unique_nodes, removed

def geo_ip(ip):
    try:
        r = requests.get(f"https://ipinfo.io/{ip}/json", timeout=5)
        if r.status_code == 200:
            data = r.json()
            cc = data.get("country")
            if cc:
                return cc.lower(), cc.upper()
    except:
        pass
    return "unknown", "UN"
    
def country_to_flag(cc):
    """Convert ISO 3166 two-letter code to emoji flag"""
    if not cc or len(cc) != 2:
        return "üè≥Ô∏è"
    return chr(0x1F1E6 + (ord(cc[0].upper()) - 65)) + chr(0x1F1E6 + (ord(cc[1].upper()) - 65))

def flag_to_country_code(flag):
    """Convert emoji flag to ISO 3166 code"""
    if not flag or len(flag) < 2:
        return None
    try:
        first, second = flag[0], flag[1]
        return chr(ord(first) - 0x1F1E6 + 65) + chr(ord(second) - 0x1F1E6 + 65)
    except:
        return None

def load_cn_to_cc():
    secret_data = os.environ.get("CN_TO_CC", "{}")
    try:
        return json.loads(secret_data)
    except Exception as e:
        print(f"[error] üò≠ Failed to parse CN_TO_CC secret: {e}")
        return {}

def build_name(flag, cc, index, ipv6_tag=False):
    suffix = " [ipv6]" if ipv6_tag else ""
    return f"{flag} {cc}-{index}{suffix} | 9PB"

# ---------------- Load sources ----------------
def load_sources():
    if not os.path.exists(SOURCES_FILE):
        print(f"[FATAL] ‚ö†Ô∏è Source not found at {SOURCES_FILE}")
        sys.exit(1)
    with open(SOURCES_FILE, "r", encoding="utf-8") as f:
        sources = [line.strip() for line in f if line.strip() and not line.startswith("#")]
    if not sources:
        print(f"[FATAL] üïµÔ∏è Source is empty. Please check the secret or file content.")
        sys.exit(1)
    return sources

# -----------------------------------------------------------
# Helper: Safe base64 decode
# -----------------------------------------------------------
def decode_b64(b64str):
    try:
        padded = b64str + "=" * (-len(b64str) % 4)
        return base64.urlsafe_b64decode(padded).decode("utf-8")
    except Exception:
        return ""

# -----------------------------------------------------------
# Helper: Generic dynamic query merger
# -----------------------------------------------------------
def merge_dynamic_fields(node, query):
    """Attach all unrecognized query fields dynamically, without injecting defaults."""
    known = set(node.keys())
    for k, v in query.items():
        if k not in known and v:  # only non-empty
            v_decoded = urllib.parse.unquote(v)
            if k == "alpn":  # only ALPN is a list
                v_list = [x.strip() for x in v_decoded.split(",") if x.strip()]
                if v_list:
                    node[k] = v_list
            else:  # everything else stays as string
                node[k] = v_decoded
    return node

# -----------------------------------------------------------
# VMESS Parser
# -----------------------------------------------------------
def parse_vmess(line):
    try:
        if not line.startswith("vmess://"):
            return None
        raw = line[8:].strip()
        data = json.loads(base64.b64decode(raw + "=" * (-len(raw) % 4)).decode("utf-8"))
        node = {
            "type": "vmess",
            "name": data.get("ps") or "",
            "server": data.get("add", ""),
            "port": int(data.get("port", 0)),
            "uuid": data.get("id", ""),
            "alterId": int(data.get("aid", 0)),
            "cipher": data.get("scy", "auto"),
            "tls": data.get("tls", "").lower() == "tls",
            "network": data.get("net", "tcp"),
        }
        if node["network"] == "ws":
            node["ws-opts"] = {
                "path": data.get("path", "/"),
                "headers": {"Host": data.get("host", "")}
            }
        # dynamic fields
        node = merge_dynamic_fields(node, data)
        return node
    except Exception as e:
        print(f"[warn] ‚ùóVmess parse error -> {e}")
        return None

# -----------------------------------------------------------
# VLESS Parser
# -----------------------------------------------------------
def parse_vless(line):
    try:
        if not line.startswith("vless://"):
            return None

        # Split name fragment
        name = ""
        if "#" in line:
            line, name = line.split("#", 1)
            name = urllib.parse.unquote(name)

        core = line[len("vless://"):]
        if "@" not in core:
            return None
        uuid, rest = core.split("@", 1)

        query = {}
        if "?" in rest:
            host_port, q = rest.split("?", 1)
            query = dict(urllib.parse.parse_qsl(q))
        else:
            host_port = rest

        if ":" not in host_port:
            return None
        host, port = host_port.split(":", 1)

        node = {
            "type": "vless",
            "name": name or "VLESS Node",
            "server": host,
            "port": int(port),
            "uuid": uuid,
            "encryption": query.get("encryption", "none"),
        }

        # Security (TLS / Reality)
        if query.get("security") == "tls":
            node["tls"] = True
            node["servername"] = query.get("sni", "")
            node["skip-cert-verify"] = query.get("allowInsecure", "0") in ("1", "true", "yes")
            if "fp" in query:
                node["client-fingerprint"] = query["fp"]
        elif query.get("security") == "reality":
            node["reality-opts"] = {
                "public-key": query.get("pbk", ""),
                "short-id": query.get("sid", ""),
                "server-name": query.get("sni", "")
            }
            node["tls"] = True

        # Network
        if "type" in query:
            node["network"] = query["type"]

        if node.get("network") == "ws":
            ws_opts = {"path": urllib.parse.unquote(query.get("path", "/"))}
            if "host" in query:
                ws_opts["headers"] = {"Host": query["host"]}
            node["ws-opts"] = ws_opts

        if node.get("network") == "grpc":
            node["grpc-opts"] = {"grpc-service-name": query.get("serviceName", "")}

        node = merge_dynamic_fields(node, query)
        return node

    except Exception as e:
        print(f"[warn] ‚ùóVLESS parse error -> {e}")
        return None

# -----------------------------------------------------------
# TROJAN Parser
# -----------------------------------------------------------
def parse_trojan(line):
    try:
        if not line.startswith("trojan://"):
            return None

        name = ""
        if "#" in line:
            line, name = line.split("#", 1)
            name = urllib.parse.unquote(name.strip())

        core = line[len("trojan://"):]
        if "@" not in core:
            return None
        password, rest = core.split("@", 1)

        query = {}
        if "?" in rest:
            host_port, q = rest.split("?", 1)
            query = dict(urllib.parse.parse_qsl(q))
        else:
            host_port = rest

        if ":" not in host_port:
            return None
        host, port = host_port.split(":", 1)

        node = {
            "type": "trojan",
            "name": name or "Trojan Node",
            "server": host.strip(),
            "port": int(port.strip()),
            "password": urllib.parse.unquote(password.strip()),
        }

        node["skip-cert-verify"] = query.get("allowInsecure", "0") in ("1", "true", "yes")
        node["security"] = query.get("security", "tls")
        sni = query.get("sni") or query.get("peer")
        if sni:
            node["sni"] = sni
            node["servername"] = sni

        if "type" in query:
            node["network"] = query["type"]

        if node.get("network") == "ws":
            ws_opts = {"path": urllib.parse.unquote(query.get("path", "/"))}
            if "host" in query:
                ws_opts["headers"] = {"Host": query["host"]}
            node["ws-opts"] = ws_opts

        elif node.get("network") == "grpc":
            node["grpc-opts"] = {"grpc-service-name": query.get("serviceName", "")}

        node = merge_dynamic_fields(node, query)
        return node
    except Exception as e:
        print(f"[warn] ‚ùóTrojan parse error -> {e}")
        return None

# -----------------------------------------------------------
# HYSTERIA2 Parser
# -----------------------------------------------------------
def parse_hysteria2(line):
    try:
        if not (line.startswith("hysteria2://") or line.startswith("hy2://")):
            return None
        if line.startswith("hy2://"):
            line = "hysteria2://" + line[len("hy2://"):]

        parsed = urllib.parse.urlparse(line)
        password = urllib.parse.unquote(parsed.username or "")
        host = parsed.hostname
        port = parsed.port
        query = dict(urllib.parse.parse_qsl(parsed.query))
        name = urllib.parse.unquote(parsed.fragment or "")

        node = {
            "type": "hysteria2",
            "name": name,
            "server": host,
            "port": int(port or 0),
            "password": password,
            "skip-cert-verify": True
        }

        if "sni" in query:
            node["tls"] = {"enabled": True, "server_name": query["sni"]}
        if "insecure" in query:
            node["tls"]["insecure"] = query["insecure"].lower() in ("1", "true", "yes")

        node = merge_dynamic_fields(node, query)
        return node
    except Exception as e:
        print(f"[warn] ‚ùóHysteria2 parse error -> {e}")
        return None

# -----------------------------------------------------------
# ANYTLS Parser
# -----------------------------------------------------------
def parse_anytls(line):
    try:
        if not line.startswith("anytls://"):
            return None
        parsed = urllib.parse.urlparse(line)
        password = urllib.parse.unquote(parsed.username or "")
        host = parsed.hostname
        port = parsed.port
        query = dict(urllib.parse.parse_qsl(parsed.query))
        name = urllib.parse.unquote(parsed.fragment or "")

        node = {
            "type": "anytls",
            "name": name,
            "server": host,
            "port": int(port or 0),
            "password": password,
        }
        node = merge_dynamic_fields(node, query)
        return node
    except Exception as e:
        print(f"[warn] ‚ùóAnytls parse error -> {e}")
        return None

# -----------------------------------------------------------
# TUIC Parser
# -----------------------------------------------------------
def parse_tuic(line):
    try:
        if not line.startswith("tuic://"):
            return None
        parsed = urllib.parse.urlparse(line)
        password = urllib.parse.unquote(parsed.username or "")
        host = parsed.hostname
        port = parsed.port
        query = dict(urllib.parse.parse_qsl(parsed.query))
        name = urllib.parse.unquote(parsed.fragment or "")
        node = {
            "type": "tuic",
            "name": name,
            "server": host,
            "port": int(port or 0),
            "password": password,
        }
        node = merge_dynamic_fields(node, query)
        return node
    except Exception as e:
        print(f"[warn] ‚ùóTUIC parse error -> {e}")
        return None

# -----------------------------------------------------------
# SHADOWSOCKS (SS) Parser
# -----------------------------------------------------------
def parse_ss(line):
    try:
        if not line.startswith("ss://"):
            return None
        line = line[5:]
        name = ""
        if "#" in line:
            line, name = line.split("#", 1)
            name = urllib.parse.unquote(name)
        plugin = None
        plugin_opts = None
        if "/?" in line:
            core, q = line.split("/?", 1)
            qd = urllib.parse.parse_qs(q)
            if "plugin" in qd:
                plugin_full = qd["plugin"][0]
                if ";" in plugin_full:
                    parts = plugin_full.split(";")
                    plugin = parts[0]
                    plugin_opts = dict(p.split("=", 1) for p in parts[1:] if "=" in p)
                else:
                    plugin = plugin_full
        else:
            core = line
        if "@" in core:
            b64, srvp = core.split("@", 1)
            decoded = decode_b64(b64)
            cipher, password = decoded.split(":", 1)
            server, port = srvp.split(":")
        else:
            decoded = decode_b64(core)
            userinfo, srvp = decoded.split("@", 1)
            cipher, password = userinfo.split(":")
            server, port = srvp.split(":")
        node = {
            "type": "ss",
            "name": name or "SS Node",
            "server": server,
            "port": int(port),
            "cipher": cipher,
            "password": password,
        }
        if plugin:
            node["plugin"] = plugin
        if plugin_opts:
            node["plugin-opts"] = plugin_opts
        return node
    except Exception as e:
        print(f"[warn] ‚ùóSS parse error -> {e}")
        return None

# -----------------------------------------------------------
# SHADOWSOCKSR (SSR) Parser
# -----------------------------------------------------------
def parse_ssr(line):
    try:
        if not line.startswith("ssr://"):
            return None
        decoded = decode_b64(line[6:])
        main, *tail = decoded.split("/?")
        parts = main.split(":")
        if len(parts) < 6:
            return None
        server, port, protocol, method, obfs, pwd_b64 = parts[:6]
        password = decode_b64(pwd_b64)
        qs = dict(urllib.parse.parse_qsl(tail[0])) if tail else {}
        node = {
            "type": "ssr",
            "name": urllib.parse.unquote(decode_b64(qs.get("remarks", [""])[0])) if "remarks" in qs else "",
            "server": server,
            "port": int(port),
            "protocol": protocol,
            "cipher": method,
            "obfs": obfs,
            "password": password
        }
        node = merge_dynamic_fields(node, qs)
        return node
    except Exception as e:
        print(f"[warn] ‚ùóSSR parse error -> {e}")
        return None

# -----------------------------------------------------------
# Dispatcher
# -----------------------------------------------------------
def parse_node_line(line):
    parsers = [
        parse_vmess,
        parse_vless,
        parse_trojan,
        parse_hysteria2,
        parse_anytls,
        parse_tuic,
        parse_ss,
        parse_ssr
    ]
    for fn in parsers:
        node = fn(line.strip())
        if node:
            return node
    return None

# ---------------- Rename node ----------------
def rename_node(p, country_counter, CN_TO_CC):
    """
    Assign a standardized name to the node without changing any other fields.
    Skip nodes with forbidden emojis or empty names.
    If USE_ONLY_GEOIP is True, assign name by GeoIP only.
    Preserves all original fields to maintain connectivity.
    """

    # Original name
    original_name = str(p.get("name", "") or "").strip()
    host = p.get("server") or p.get("add") or ""

    # Detect ipv6 tag
    ipv6_tag = False
    if re.search(r'[\(\[\{]?\s*ipv6\s*[\)\]\}]?', original_name, flags=re.IGNORECASE):
        ipv6_tag = True

    # Define forbidden emojis (any emoji you want to filter out)
    FORBIDDEN_EMOJIS = {"üîí", "‚ùå", "‚ö†Ô∏è", "üöÄ", "üéÅ"}

    # Extract grapheme clusters (so multi-codepoint emojis like ‚ö†Ô∏è are kept together)
    graphemes = list(original_name)

    # Skip nodes with empty names or containing any forbidden emoji
    if any(g in FORBIDDEN_EMOJIS for g in graphemes):
        return None

    # ----------If GEOIP-ONLY Mode Is Set----------
    if USE_ONLY_GEOIP:
    
        # 1Ô∏è‚É£ GeoIP first
        ip = resolve_ip(host) or host
        cc = flag = None
        cc_lower, cc_upper = geo_ip(ip)
    
        if cc_upper and cc_upper != "UN":
            cc = cc_upper
            flag = country_to_flag(cc)
    
        # Prepare name once
        name_for_match = unquote(original_name)
    
        # 2Ô∏è‚É£ Chinese mapping
        if not cc:
            for cn_name, code in CN_TO_CC.items():
                if cn_name and cn_name in name_for_match:
                    cc = code.upper()
                    flag = country_to_flag(cc)
                    break
    
        # 3Ô∏è‚É£ Emoji flag
        if not cc:
            flag_match = re.search(r'[\U0001F1E6-\U0001F1FF]{2}', name_for_match)
            if flag_match:
                flag = flag_match.group(0)
                cc = flag_to_country_code(flag)
                if cc:
                    cc = cc.upper()
    
        # 4Ô∏è‚É£ Two-letter ISO code (context-aware, unit-safe)
        if not cc:
            iso_iter = re.finditer(r'\b([A-Z]{2})\b', original_name)
        
            for iso_match in iso_iter:
                iso = iso_match.group(1)
        
                before = original_name[:iso_match.start()]
        
                # Reject units like "100GB" or "100 GB"
                if re.search(r'\d\s*$', before):
                    continue
        
                cc = iso
                flag = country_to_flag(cc)
    
        if not cc:
            return None    # ‚ùå truly unnameable ‚Üí skip
    
    # ----------If GEOIP-ONLY Mode Is Not Set----------
    else:
        name_for_match = unquote(original_name)
        cc = flag = None
    
        # 1Ô∏è‚É£ Chinese mapping
        for cn_name, code in CN_TO_CC.items():
            if cn_name and cn_name in name_for_match:
                cc = code.upper()
                flag = country_to_flag(cc)
                break
    
        # 2Ô∏è‚É£ Emoji flag
        if not cc:
            flag_match = re.search(r'[\U0001F1E6-\U0001F1FF]{2}', name_for_match)
            if flag_match:
                flag = flag_match.group(0)
                cc = flag_to_country_code(flag)
                if cc:
                    cc = cc.upper()
    
        # 3Ô∏è‚É£ Two-letter ISO code (unit-safe)
        if not cc:
            iso_iter = re.finditer(r'\b([A-Z]{2})\b', original_name)
        
            for iso_match in iso_iter:
                iso = iso_match.group(1)
        
                before = original_name[:iso_match.start()]
        
                # Reject units like "100GB" or "100 GB"
                if re.search(r'\d\s*$', before):
                    continue
        
                cc = iso
                flag = country_to_flag(cc)
    
        # 4Ô∏è‚É£ GeoIP fallback
        if not cc:
            ip = resolve_ip(host) or host
            cc_lower, cc_upper = geo_ip(ip)
            if cc_upper and cc_upper != "UN":
                cc = cc_upper
                flag = country_to_flag(cc)
    
        if not cc:
            return None    # ‚ùå truly unnameable ‚Üí skip
    
        # ----------Final naming----------
        country_counter[cc] += 1
        index = country_counter[cc]
        p["name"] = build_name(flag, cc, index, ipv6_tag)
        return p

# ---------------- Load proxies ----------------
def load_proxies(url, retries=5):
    headers = {
        "User-Agent": "Clash/1.18 (GitHub Actions; Python)",
        "Accept": "*/*",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "close",
    }

    for attempt in range(1, retries + 1):
        try:
            resp = requests.get(url, headers=headers, timeout=20)
            status = resp.status_code

            if status in (401, 403):
                print(f"[FATAL] üö´ HTTP {status} Forbidden ‚Äî subscription blocked in CI", flush=True)
                sys.exit(1)

            if status != 200:
                raise RuntimeError(f"HTTP {status}")

            text = resp.text.strip()
            if not text:
                raise RuntimeError("empty response")

            print(f"[fetch] üì• HTTP 200, {len(text)} chars fetched", flush=True)

            # -------- base64 auto detect --------
            if (
                len(text.splitlines()) == 1
                and re.fullmatch(r"[A-Za-z0-9+/=_-]+", text)
            ):
                try:
                    decoded = base64.b64decode(text + "=" * (-len(text) % 4)).decode("utf-8")
                    text = decoded
                    print(f"[decode] üîì base64 decoded -> {len(text.splitlines())} lines", flush=True)
                except Exception:
                    print("[warn] ‚ö†Ô∏è base64 detected but decode failed", flush=True)

            nodes = []

            # -------- YAML --------
            if "proxies:" in text:
                data = yaml.safe_load(text)
                for p in data.get("proxies", []):
                    nodes.append(p)
                return nodes

            # -------- URI lines --------
            for line in text.splitlines():
                line = line.strip()
                if not line:
                    continue
                node = parse_node_line(line)
                if node:
                    nodes.append(node)

            return nodes

        except Exception as e:
            print(f"[warn] üò≠ fetch failed ({attempt}/{retries}) -> {e}", flush=True)
            time.sleep(2)

    print("[abort] üö´ All retries failed, aborting", flush=True)
    sys.exit(1)

# ---------------- Main ----------------
def main():
    try:
        sources = load_sources()
        print(f"[start] üñ•Ô∏è Loaded {len(sources)} subscription links from source")

        all_nodes = []
        for url in sources:
            nodes = load_proxies(url)
            print(f"[source] üìù {len(nodes)} nodes parsed from current subscription")
            all_nodes.extend(nodes)

        print(f"[collect] üìã Total {len(all_nodes)} nodes successfully parsed from all subscriptions")

        # ---------------- Latency filter ----------------
        if USE_LATENCY:
            print(f"[latency] üö´ Filtering nodes > {LATENCY_THRESHOLD} ms")
            country_counter = defaultdict(int)
            filtered_nodes = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=50) as ex:
                futures = [ex.submit(tcp_latency_ms, n.get("server"), n.get("port")) for n in all_nodes]
                for n, f in zip(all_nodes, futures):
                    latency = f.result()
                    if latency <= LATENCY_THRESHOLD:
                        filtered_nodes.append(n)

            num_filtered = len(all_nodes) - len(filtered_nodes)
            print(f"[latency] ‚ùóFiltered {num_filtered} nodes due to latency")
            print(f"[latency]  üñ®Ô∏è Total {len(filtered_nodes)} nodes remain after latency filtering")
        else:
            filtered_nodes = all_nodes
            country_counter = defaultdict(int)
            print(f"[latency] üöÄ Latency filtering disabled, {len(filtered_nodes)} nodes remain")

        # ---------------- Duplicate filter ----------------
        if USE_DUPLICATE_FILTER:
            print("[dedup] üßπ Removing duplicate nodes (server + port + uuid/password)")
            before = len(filtered_nodes)
            filtered_nodes, removed = deduplicate_nodes(filtered_nodes)
            after = len(filtered_nodes)
            print(f"[dedup] ¬ÆÔ∏èemoved {removed} duplicate nodes")
            print(f"[dedup] üñ®Ô∏è Total {after} nodes remain after deduplication")
        else:
            print("[dedup] üàÅ Duplicate filtering disabled")

        # ---------------- Renamed nodes ----------------
        renamed_nodes = []
        cn_to_cc = load_cn_to_cc()
        skipped_nodes = 0
        for n in filtered_nodes:
            res = rename_node(n, country_counter, cn_to_cc)
            if res:
                renamed_nodes.append(res)
            else:
                skipped_nodes += 1

        if skipped_nodes > 0:
            print(f"[rename] ‚ö†Ô∏è Skipped {skipped_nodes} nodes that could not be assigned a name or include forbidden emoji")
        print(f"[rename] üñ®Ô∏è Final {len(renamed_nodes)} nodes remain after name correction")

        if not renamed_nodes:
            print("[FATAL] üÖæÔ∏è valid nodes after processing. Abort upload.")
            sys.exit(1)

        # ---------------- Load template ----------------
        try:
            r = requests.get(TEMPLATE_URL, timeout=15)
            r.raise_for_status()
            template_text = r.text
        except Exception as e:
            print(f"[FATAL] ‚ö†Ô∏è Failed to fetch ClashTemplate -> {e}")
            sys.exit(1)

        # ---------------- Preferred key order ----------------
        INFO_ORDER = [
            "name", "type", "server", "port", "uuid", "password",
            "encryption", "network", "security", "sni", "servername",
            "skip-cert-verify", "fp", "client-fingerprint",
            "path", "ws-opts", "grpc-opts", "h2-opts"
        ]
        
        # ---------------- Function to reorder keys ----------------
        def reorder_info(node):
            ordered = OrderedDict()
            # Add preferred keys only if they exist in the node
            for key in INFO_ORDER:
                if key in node:
                    val = node[key]
                    # Convert string to list only if original value is a list or comma string
                    if key in ("alpn") and isinstance(val, str):
                        # Only split if val is not empty
                        val_list = [x.strip() for x in val.split(",") if x.strip()]
                        ordered[key] = val_list if val_list else val
                    else:
                        ordered[key] = val
            # Append extra keys not in preferred order
            for key in node:
                if key not in ordered:
                    ordered[key] = node[key]
            return ordered
        
        # Apply to all renamed nodes
        info_ordered = [reorder_info(n) for n in renamed_nodes]
        info_ordered_dicts = [dict(n) for n in info_ordered]

        # Line by line YAML proxies output format
        def make_single_line_yaml(proxies):
            lines = []
            for p in proxies:
                # Convert nested dicts safely
                def to_yaml_value(v):
                    if isinstance(v, dict):
                        inner = ", ".join(f"{k}: {json.dumps(vv, ensure_ascii=False)}" for k, vv in v.items())
                        return "{" + inner + "}"
                    else:
                        return json.dumps(v, ensure_ascii=False)
        
                parts = []
                for k, v in p.items():
                    parts.append(f"{k}: {to_yaml_value(v)}")
        
                line = "- {" + ", ".join(parts) + "}"
                lines.append(line)
        
            return "\n".join(lines)

        # ---------------- Convert to YAML ----------------
        proxies_yaml_block = make_single_line_yaml(info_ordered_dicts)    #If multiple lines format is needed, Delete Line by line YAML proxies output format code block, proxies_yaml_block = yaml.dump(info_ordered_dicts, allow_unicode=True, default_flow_style=False, sort_keys=False)
        proxy_names_block = "\n".join([f"      - {unquote(p['name'])}" for p in info_ordered_dicts])

        # ---------------- Replace placeholders ----------------
        output_text = template_text.replace("{{PROXIES}}", proxies_yaml_block)
        output_text = output_text.replace("{{PROXY_NAMES}}", proxy_names_block)

        # ---------------- Prepare timestamp ----------------
        offset = timedelta(hours=6, minutes=30)
        utc_now = datetime.now(timezone.utc)
        local_time = utc_now + offset
        timestamp = local_time.strftime("%d.%m.%Y %H:%M:%S")

        # ---------------- Write output ----------------
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(f"# Last update: {timestamp}\n" + output_text)
        print(f"[done] üíæ Wrote {OUTPUT_FILE}")

        # Upload to textdb only after all upper processes successful processing
        upload_to_textdb()

    except Exception as e:
        print("[‚ö†Ô∏èFATAL ERROR in main]", str(e))
        traceback.print_exc()
        sys.exit(1)

# ---------------- Upload to TextDB ----------------
def upload_to_textdb():
    try:
        # Step 1: Read freshly generated Filter file (local, not GitHub raw)
        with open("9PB", "r", encoding="utf-8") as f:
            output_text = f.read()

        # Step 2: Delete old data
        delete_resp = requests.post(TEXTDB_API, data={"value": ""})
        if delete_resp.status_code == 200:
            print("[info] üóëÔ∏è Successfully deleteded old data on textdb")
        else:
            print(f"[warn] ‚ùå Failed to delete old data on textdb: {delete_resp.status_code}")
            print(f"[warn] ‚ùóResponse: {delete_resp.text}")

        # Wait 3 seconds
        time.sleep(3)

        # Step 3: Upload new data
        upload_resp = requests.post(TEXTDB_API, data={"value": output_text})
        if upload_resp.status_code == 200:
            print("[info] üì§ Successfully uploaded new data on textdb")
        else:
            print(f"[warn] ‚ùåFailed to upload new data on textdb: {upload_resp.status_code}")
            print(f"[warn] ‚ùóResponse: {upload_resp.text}")

    except Exception as e:
        print(f"[error] ‚õî Unexpected error: {e}")

# ---------------- Entry ----------------
if __name__ == "__main__":
    main()
